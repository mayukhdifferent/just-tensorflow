{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVWVpD9KSGSg"
   },
   "source": [
    "# PART I\n",
    "* Read data\n",
    "* Divide on train and test data\n",
    "* Replace default ids with names\n",
    "* Extend word dictionary by 3 places, to add special characters\n",
    "* Decoding function\n",
    "* Give each review same length (the preprocessing thing)\n",
    "* Define layers of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8foJyZjH33A"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "data = keras.datasets.imdb\n",
    "\n",
    "(train_img, train_labels), (test_img, test_labels) = data.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPG4oC9nIiG_",
    "outputId": "88fa24a9-e386-4c03-fed1-612cf54a52ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842\n"
     ]
    }
   ],
   "source": [
    "word_dir = data.get_word_index()\n",
    "\n",
    "print(word_dir['whatever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QvJUaqzHI5l_",
    "outputId": "44d84c44-e53f-421c-b46e-d1082c0f23ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn   34701\n"
     ]
    }
   ],
   "source": [
    "for word, id in word_dir.items():\n",
    "  print(word, \" \", id)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMDm2OYfJ0HE"
   },
   "outputs": [],
   "source": [
    "word_dir = { name: (id+3) for name, id in word_dir.items()} # extending the dir by 3 places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QHMykr2TJ-xq",
    "outputId": "0941c40d-6fee-4ca4-e8a1-e1b3c226b48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n"
     ]
    }
   ],
   "source": [
    "print(word_dir['whatever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EarVW0kK_ZT"
   },
   "outputs": [],
   "source": [
    "word_dir['[ADD]'] = 0 # additional character, if the movie review is too short\n",
    "word_dir['[START]'] = 1\n",
    "word_dir['[UNKNOWN]'] = 2\n",
    "word_dir['[UNUSED]'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YICGoyJgLfPA"
   },
   "outputs": [],
   "source": [
    "# reversing dictionary\n",
    "# the word_dir is \"word\":123, but we want 123:\"word\" pointer\n",
    "\n",
    "word_dir_rev = dict([(id, name) for name, id in word_dir.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jfd6cSpMNGLb",
    "outputId": "daebda8d-9275-4806-e799-66a6302c15d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whatever\n"
     ]
    }
   ],
   "source": [
    "print(word_dir_rev[845])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDFOZp6tNcCq"
   },
   "outputs": [],
   "source": [
    "def decoding(text):\n",
    "  text_to_return = \"\"\n",
    "  for word in text:\n",
    "    text_to_return += \" \" + word_dir_rev[word]\n",
    "  return text_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UBk696S3Ojee",
    "outputId": "f23e0f72-bcca-4be5-8fd7-202d73f4ca88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" [START] please give this one a miss br br [UNKNOWN] [UNKNOWN] and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite [UNKNOWN] so all you madison fans give this a miss\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding(test_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cN9GG1eNOoXu"
   },
   "outputs": [],
   "source": [
    "# giving the reviews same shape - 250 charas\n",
    "# (...).pad_sequences(data, maxlen, value, padding)\n",
    "# |data| - data to alter, |maxlen| - max. length of each review, |value| - which value should be the review filled with,\n",
    "# in case its too short, |padding| - should it appear on beginning or end of the review\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_img, maxlen=250, value=word_dir['[ADD]'], padding=\"post\")\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_img, maxlen=250, value=word_dir['[ADD]'], padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "VWFjRbMSPfO0",
    "outputId": "56634fc2-d07b-4cbb-db8d-0f90bfa59029"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" [START] please give this one a miss br br [UNKNOWN] [UNKNOWN] and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite [UNKNOWN] so all you madison fans give this a miss [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD] [ADD]\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weJ4SGIDPkNK"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(10000, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# PART 2 SAVING MODEL\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.0991 - acc: 0.9694 - val_loss: 0.3392 - val_acc: 0.8791\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 2s 131us/sample - loss: 0.0805 - acc: 0.9778 - val_loss: 0.3572 - val_acc: 0.8784\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 2s 153us/sample - loss: 0.0666 - acc: 0.9835 - val_loss: 0.3849 - val_acc: 0.8749\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.0533 - acc: 0.9875 - val_loss: 0.4179 - val_acc: 0.8715\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0430 - acc: 0.9911 - val_loss: 0.4612 - val_acc: 0.8681\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0338 - acc: 0.9938 - val_loss: 0.4917 - val_acc: 0.8673\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.0268 - acc: 0.9955 - val_loss: 0.5323 - val_acc: 0.8655\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.0206 - acc: 0.9972 - val_loss: 0.5724 - val_acc: 0.8646\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 2s 121us/sample - loss: 0.0156 - acc: 0.9981 - val_loss: 0.6185 - val_acc: 0.8637\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.0118 - acc: 0.9987 - val_loss: 0.6661 - val_acc: 0.8600\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0091 - acc: 0.9992 - val_loss: 0.7005 - val_acc: 0.8615\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.0073 - acc: 0.9993 - val_loss: 0.7444 - val_acc: 0.8588\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 2s 122us/sample - loss: 0.0052 - acc: 0.9997 - val_loss: 0.7764 - val_acc: 0.8589\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.0037 - acc: 0.9997 - val_loss: 0.8195 - val_acc: 0.8590\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.8566 - val_acc: 0.8565\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 2s 132us/sample - loss: 0.0019 - acc: 0.9999 - val_loss: 0.8922 - val_acc: 0.8579\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.9349 - val_acc: 0.8577\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.9712 - val_acc: 0.8575\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.0012 - acc: 0.9998 - val_loss: 1.0045 - val_acc: 0.8548\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 7.2392e-04 - acc: 1.0000 - val_loss: 1.0433 - val_acc: 0.8531\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.0010 - acc: 0.9999 - val_loss: 1.0792 - val_acc: 0.8533\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 5.9742e-04 - acc: 0.9999 - val_loss: 1.1130 - val_acc: 0.8526\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 2s 132us/sample - loss: 4.1427e-04 - acc: 0.9999 - val_loss: 1.1415 - val_acc: 0.8532\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 2.9671e-04 - acc: 1.0000 - val_loss: 1.1681 - val_acc: 0.8538\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 2s 145us/sample - loss: 2.7634e-04 - acc: 1.0000 - val_loss: 1.1979 - val_acc: 0.8511\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 2.1637e-04 - acc: 1.0000 - val_loss: 1.2257 - val_acc: 0.8526\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 2s 150us/sample - loss: 0.0013 - acc: 0.9996 - val_loss: 1.2758 - val_acc: 0.8484\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 3s 170us/sample - loss: 4.3730e-04 - acc: 1.0000 - val_loss: 1.2862 - val_acc: 0.8519\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 2s 166us/sample - loss: 1.8415e-04 - acc: 1.0000 - val_loss: 1.3117 - val_acc: 0.8521\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 2s 166us/sample - loss: 1.0821e-04 - acc: 1.0000 - val_loss: 1.3182 - val_acc: 0.8516\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 2s 155us/sample - loss: 1.5053e-04 - acc: 1.0000 - val_loss: 1.3361 - val_acc: 0.8516\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 2s 150us/sample - loss: 8.4950e-05 - acc: 1.0000 - val_loss: 1.3521 - val_acc: 0.8525\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 6.8055e-05 - acc: 1.0000 - val_loss: 1.3672 - val_acc: 0.8518\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 4.5367e-05 - acc: 1.0000 - val_loss: 1.3856 - val_acc: 0.8515\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 3s 171us/sample - loss: 2.9484e-04 - acc: 0.9999 - val_loss: 1.4085 - val_acc: 0.8513\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 2s 146us/sample - loss: 4.5774e-05 - acc: 1.0000 - val_loss: 1.4193 - val_acc: 0.8518\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 2s 161us/sample - loss: 3.5371e-05 - acc: 1.0000 - val_loss: 1.4339 - val_acc: 0.8509\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 2s 152us/sample - loss: 3.4679e-05 - acc: 1.0000 - val_loss: 1.4504 - val_acc: 0.8506\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 2s 149us/sample - loss: 2.3765e-05 - acc: 1.0000 - val_loss: 1.4663 - val_acc: 0.8515\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 2s 145us/sample - loss: 2.0593e-05 - acc: 1.0000 - val_loss: 1.4843 - val_acc: 0.8516\n"
     ]
    }
   ],
   "source": [
    "fitModel = model.fit(x_train, y_train, epochs=40, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 47us/sample - loss: 1.5973 - acc: 0.8366\n",
      "[1.5973482226109506, 0.83664]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(text):\n",
    "    encoded = [1]\n",
    "    \n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word in word_dir:\n",
    "            encoded.append(word_dir[word])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop with the nonsense. This show is hilarious. If they used the same cast or facsimiles of them, it wouldn't be a remake. It would be the British version. I've seen both versions, and each has its own charm, style, and tense scenarios. Ricky Gervais, who created the series, is a co-producer and writes some episodes, which is ironic considering he took the biggest part in the first, three episodes (to get it off its feet) and they have been trashed the most.\n",
      "\n",
      "[[   6 1479 1983    2  590  830 5135 2014  963    2    6 3363 1095    2\n",
      "   830  963 3363  590  963    2  590 1148  963 3363    6 1479   13 1604\n",
      "   590    2    2 1479   13 1148 2295 5135    2 1331  963 1479 1964    6\n",
      "    13  590    2 1992 2023 1604    2 1148 1479  963    6  830  963 1095\n",
      "     2  830 2023  963    2  590  963 1479   13  963  590    2   13  590\n",
      "     2    6    2 1148 1604 1657 1479 1604 1095 1206 1148  963 1479    2\n",
      "     6 3363 1095    2 1992 1479   13  830  963  590    2  590 1604 1983\n",
      "   963    2  963 1657   13  590 1604 1095  963  590    2 1992 2023   13\n",
      "  1148 2023    2   13  590    2   13 1479 1604 3363   13 1148    2 1148\n",
      "  1604 3363  590   13 1095  963 1479   13 3363 1331    2 2023  963    2\n",
      "   830 1604 1604 2295    2  830 2023  963    2  503   13 1331 1331  963\n",
      "   590  830    2 1657    6 1479  830    2   13 3363    2  830 2023  963\n",
      "     2 1209   13 1479  590  830    2  830 2023 1479  963  963    2  963\n",
      "  1657   13  590 1604 1095  963  590    2  830 1604    2 1331  963  830\n",
      "     2   13  830    2 1604 1209 1209    2   13  830  590    2 1209  963\n",
      "   963  830    2    6 3363 1095    2  830 2023  963 5135    2 2023    6\n",
      "  1964  963    2  503  963  963 3363    2  830 1479    6  590 2023  963\n",
      "  1095    2  830 2023  963    2 1983 1604  590  830    2    2]]\n",
      "[0.]\n",
      "It's clear that Steve C. (I don't want to butcher the spelling of his last name) either took tips from Ricky Gervais or studied his performance. They even readjust their tie the same way. Albeit, the first two seasons were a little weak as the show gained steam and attempted to match its predecessor, but that's no reason to shrug it off. From the third season onward, the \"American\" Office has come into its own with story lines that introduced new characters and shook the formula of a nerdy boy awkwardly pining for a pretty girl.\n",
      "\n",
      "[[   6  830 1148 2023    2   13  830  590    2 1657 1479  963 1095  963\n",
      "  1148  963  590  590 1604 1479    2  503 1206  830    2  830 2023    6\n",
      "   830  590    2 3363 1604    2 1479  963    6  590 1604 3363    2  830\n",
      "  1604    2  590 2023 1479 1206 1331    2   13  830    2 1604 1209 1209\n",
      "     2    2 1209 1479 1604 1983    2  830 2023  963    2  830 2023   13\n",
      "  1479 1095    2  590  963    6  590 1604 3363    2 1604 3363 1992    6\n",
      "  1479 1095    2  830 2023  963    2    6 1983  963 1479   13 1148    6\n",
      "  3363    2 1604 1209 1209   13 1148  963    2 2023    6  590    2 1148\n",
      "  1604 1983  963    2   13 3363  830 1604    2   13  830  590    2 1604\n",
      "  1992 3363    2 1992   13  830 2023    2  590  830 1604 1479 5135    2\n",
      "  2014   13 3363  963  590    2  830 2023    6  830    2   13 3363  830\n",
      "  1479 1604 1095 1206 1148  963 1095    2 3363  963 1992    2 1148 2023\n",
      "     6 1479    6 1148  830  963 1479  590    2    6 3363 1095    2  590\n",
      "  2023 1604 1604 2295    2  830 2023  963    2 1209 1604 1479 1983 1206\n",
      "  2014    6    2 1604 1209    2    6    2 3363  963 1479 1095 5135    2\n",
      "   503 1604 5135    2    6 1992 2295 1992    6 1479 1095 2014 5135    2\n",
      "  1657   13 3363   13 3363 1331    2 1209 1604 1479    2    6    2 1657\n",
      "  1479  963  830  830 5135    2 1331   13 1479 2014    2    2]]\n",
      "[0.9995067]\n",
      "Every fan should be happy to have new episodes of a great show that would otherwise be off the air. For a true fan of the Office that should be enough, but since it's labeled \"American\" some immediately set out to find the wrong. Cheers to the people that were capable of giving the remake a chance and didn't base their opinion on what country it's filmed in. All I ask of those whom originally ripped the \"American\" version is to go online and watch the seasons with Ed Helms as Andy, a kiss-ass with anger management issues. The feud with Dwight and he (once the branches merge) has become my favorite storyline of the show, culminating in a duel. It's classic.\n",
      "[[ 963    2    6 1983  963 1479   13 1148    6 3363    2 1964  963 1479\n",
      "   590   13 1604 3363    2   13  590    2  830 1604    2 1331 1604    2\n",
      "  1604 3363 2014   13 3363  963    2    6 3363 1095    2 1992    6  830\n",
      "  1148 2023    2  830 2023  963    2  590  963    6  590 1604 3363  590\n",
      "     2 1992   13  830 2023    2  963 1095    2 2023  963 2014 1983  590\n",
      "     2    6  590    2    6 3363 1095 5135    2    6    2 2295   13  590\n",
      "   590    6  590  590    2 1992   13  830 2023    2    6 3363 1331  963\n",
      "  1479    2 1983    6 3363    6 1331  963 1983  963 3363  830    2   13\n",
      "   590  590 1206  963  590    2    2  830 2023  963    2 1209  963 1206\n",
      "  1095    2 1992   13  830 2023    2 1095 1992   13 1331 2023  830    2\n",
      "     6 3363 1095    2 2023  963    2 1604 3363 1148  963    2  830 2023\n",
      "   963    2  503 1479    6 3363 1148 2023  963  590    2 1983  963 1479\n",
      "  1331  963    2 2023    6  590    2  503  963 1148 1604 1983  963    2\n",
      "  1983 5135    2 1209    6 1964 1604 1479   13  830  963    2  590  830\n",
      "  1604 1479 5135 2014   13 3363  963    2 1604 1209    2  830 2023  963\n",
      "     2  590 2023 1604 1992    2 1148 1206 2014 1983   13 3363    6  830\n",
      "    13 3363 1331    2   13 3363    2    6    2 1095 1206  963 2014    2\n",
      "     2   13  830  590    2 1148 2014    6  590  590   13 1148]]\n",
      "[1.7363158e-07]\n"
     ]
    }
   ],
   "source": [
    "with open(\"review.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        nline = (line.replace(\",\", \"\").replace(\".\", \" \")\n",
    "        .replace(\"(\", \"\").replace(\":\", \"\").replace(\";\", \"\").replace(\")\", \"\")\n",
    "        .replace(\"'\", \"\").replace('\"', \"\").replace(\"-\", \"\").strip(\" \"))\n",
    "        encode = review_encode(nline)\n",
    "        encode = keras.preprocessing.sequence.pad_sequences([encode],\n",
    "                                                   value=word_dir['[ADD]'],\n",
    "                                                   padding=\"post\",\n",
    "                                                   maxlen=250)\n",
    "        predict = model.predict(encode)\n",
    "        print(line)\n",
    "        print(encode)\n",
    "        print(predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Text_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
